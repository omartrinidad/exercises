\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{titlesec}

% Avoid section numbers
\titleformat{\section}
{\normalfont\Large\bfseries}   % The style of the section title
{}                             % a prefix
{0pt}                          % How much space exists between the prefix and the title
{}    % How the section is represented

\begin{document}
 
\title{Sheet 4} 
\maketitle
 
\section{Assignment 19}

\section{Assignment 20}

Given an array of patterns \texttt{P} with \texttt{n} elements and a function called
\texttt{random} useful to calculate integer random number between a range it is
possible to shuffle the original array using the next algorithm:

% pseudocode
\begin{algorithm}
\caption*{Randomize elements in an array}
    \begin{algorithmic}[1]
        \For{\texttt{$i = 0$ such that $i < n$}}
            \State $i\gets i + 1$
            \State $swap\gets random(0, n)$
            \State $temp\gets P[swap]$
            \State $P[swap]\gets P[i]$
            \State $P[i]\gets temp$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Assignment 21}

\textbf{Backpropagation} of errors (BP). After the computation of the output
values, we calculate the difference between our teacher value
$^{p}{\hat{y}}_{m}$ and each output unit $^{p}y_{m}$. We obtain the sum of all
these differences and apply to it an error function:

$$ ^{p}E = \frac{1}{2} \sum_{m=1}^{M} (^{p}{\hat{y}}_{m} -\: ^{p}y_{m})^2 $$

Now, with a derivation of the error function (ommited here) is possible to get
a new formula to calculate $\delta\: values$. This calculation is different for
the neurons located in the output layer: 

$$\delta_{m} = ( \hat{y}_{m} -\: y_{m}) \cdot f'(net_{m})$$

and the neurons located in the hidden layers: 

$$ \delta_{h} = (\sum_{k=1}^{k} \delta_{k} w_{hk}) \cdot f'(net_{h}) $$

We must be warned that $\delta_{k}$ and $w_{hk}$ are referring to values in the
next layer. Now, using the $\delta\: value$, the output $\widetilde{out_{g}}$,
and a learning rate $\eta$ we can calculate the weight changes is using
this formula:

$$ \Delta w_{ij} = \eta \cdot \delta w_{ij} \cdot \widetilde{out_{g}} $$

We iterate in this process until we reach the input layer. Finally, we update
the current weights $w_{ij}$ adding to them the changed weights $\Delta w_{ij} $:

$$ w_{ij} = w_{ij} + \Delta w_{ij} $$

\section{Assignment 22}

\section{Assignment 23}

 
\end{document}
